{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "name": "Assignment 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhm4/DeepLearning-task/blob/master/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAebFT1zxhas",
        "colab_type": "text"
      },
      "source": [
        "###############################################################################################################################\n",
        "##ASSIGNMENT_02 _IDL\n",
        "\n",
        "\n",
        "###############################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQepi3_fxhav",
        "colab_type": "text"
      },
      "source": [
        "### task_01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AeBRoK4xhax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "\n",
        "\n",
        "# get the data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                    batch_size=256, seed=int(time()))\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# this is a super deep model, cool!\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "w_range = 0.4\n",
        "\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDw8Aodoxha7",
        "colab_type": "text"
      },
      "source": [
        "# FAIL_01\n",
        "\n",
        "##### Observations:\n",
        "* The Loss function is leading to large value of \"nan\"\n",
        "* Accuracy is lowest\n",
        "\n",
        "\n",
        "#### Possible_fixes\n",
        "* w_range =0.0001 gives minimum loss \n",
        "* with only \"2\" layers accuracy raises to 95-96 % and loss reaches\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJeeWOrwxha8",
        "colab_type": "code",
        "colab": {},
        "outputId": "351c572d-ba85-4927-8caf-8a642cb81477"
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEJEG41yxhbG",
        "colab_type": "code",
        "colab": {},
        "outputId": "15c32717-c37b-454d-cca7-d63bef325fa7"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 32648), started 1:37:06 ago. (Use '!kill 32648' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-ac4abab8675a3122\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-ac4abab8675a3122\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 6006;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV2qKCYyxhbQ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### task_02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fKZQwoJ-xhbS",
        "colab_type": "code",
        "colab": {},
        "outputId": "a945c492-db5c-4e09-f627-2b2f47cc0544"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "\n",
        "\n",
        "# get the data\n",
        "\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256, seed=int(time()))\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# this is a super deep model, cool!\n",
        "n_units = 100\n",
        "n_layers = 8\n",
        "w_range = 0.1\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.sigmoid,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-w_range,\n",
        "                                                         maxval=w_range)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.302588701248169 Accuracy: 0.0546875\n",
            "Loss: 2.3014566898345947 Accuracy: 0.0859375\n",
            "Loss: 2.2999491691589355 Accuracy: 0.1328125\n",
            "Starting new epoch...\n",
            "Loss: 2.299797296524048 Accuracy: 0.125\n",
            "Loss: 2.3034958839416504 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.3005361557006836 Accuracy: 0.12109375\n",
            "Loss: 2.296292304992676 Accuracy: 0.1328125\n",
            "Loss: 2.2952463626861572 Accuracy: 0.12109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2966763973236084 Accuracy: 0.1171875\n",
            "Loss: 2.287877321243286 Accuracy: 0.13671875\n",
            "Starting new epoch...\n",
            "Loss: 2.2283103466033936 Accuracy: 0.19921875\n",
            "Loss: 1.8669496774673462 Accuracy: 0.2734375\n",
            "Starting new epoch...\n",
            "Loss: 2.2528600692749023 Accuracy: 0.1796875\n",
            "Loss: 1.3025857210159302 Accuracy: 0.45703125\n",
            "Loss: 1.0507081747055054 Accuracy: 0.58984375\n",
            "Starting new epoch...\n",
            "Loss: 0.8838106393814087 Accuracy: 0.66796875\n",
            "Loss: 0.8102004528045654 Accuracy: 0.7890625\n",
            "Starting new epoch...\n",
            "Loss: 0.4216696619987488 Accuracy: 0.88671875\n",
            "Loss: 0.2864460349082947 Accuracy: 0.9375\n",
            "Starting new epoch...\n",
            "Loss: 0.2256394922733307 Accuracy: 0.9296875\n",
            "Final test accuracy: 0.9275000095367432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEIwFdKmxhbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRgU-JPDxhbg",
        "colab_type": "code",
        "colab": {},
        "outputId": "4af8dfd4-4bc9-4dc2-f74b-5867c5fa90c9"
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 32648), started 14:27:50 ago. (Use '!kill 32648' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-8fe98118c54527a1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-8fe98118c54527a1\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 6006;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXyfnSW6xhbo",
        "colab_type": "text"
      },
      "source": [
        "# FAIL_02\n",
        "\n",
        "\n",
        "#### Observation\n",
        "* loss function is shows very minimal change\n",
        "* grads in hidden layer are convering to infinitesimally smaller values, vanishing\n",
        "* activation function is sigmoidal\n",
        "\n",
        "\n",
        "\n",
        "#### Possible_fixes\n",
        "* activation function ReLu instead of sigmoidal, acc of 92% could be achieved\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehYd7fQixhbq",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### task_03"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrOr29ODxhbr",
        "colab_type": "code",
        "colab": {},
        "outputId": "676fd78d-a6fe-40f8-9fd6-b9836c2d58eb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime \n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "\n",
        "\n",
        "# get the data\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256, seed=int(time()))\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.302584409713745 Accuracy: 0.109375\n",
            "Loss: 2.302786350250244 Accuracy: 0.08203125\n",
            "Loss: 2.2919421195983887 Accuracy: 0.1875\n",
            "Starting new epoch...\n",
            "Loss: 2.3001952171325684 Accuracy: 0.0859375\n",
            "Loss: 2.3054590225219727 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.297451972961426 Accuracy: 0.16015625\n",
            "Loss: 2.3041024208068848 Accuracy: 0.08984375\n",
            "Loss: 2.3011035919189453 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.2965352535247803 Accuracy: 0.12890625\n",
            "Loss: 2.3081459999084473 Accuracy: 0.07421875\n",
            "Starting new epoch...\n",
            "Loss: 2.2942192554473877 Accuracy: 0.13671875\n",
            "Loss: 2.3003830909729004 Accuracy: 0.109375\n",
            "Starting new epoch...\n",
            "Loss: 2.3007216453552246 Accuracy: 0.0859375\n",
            "Loss: 2.3001747131347656 Accuracy: 0.11328125\n",
            "Loss: 2.2988152503967285 Accuracy: 0.1328125\n",
            "Starting new epoch...\n",
            "Loss: 2.3033738136291504 Accuracy: 0.078125\n",
            "Loss: 2.305172920227051 Accuracy: 0.09765625\n",
            "Starting new epoch...\n",
            "Loss: 2.300079345703125 Accuracy: 0.13671875\n",
            "Loss: 2.3026082515716553 Accuracy: 0.09375\n",
            "Starting new epoch...\n",
            "Loss: 2.2971880435943604 Accuracy: 0.13671875\n",
            "Final test accuracy: 0.11349999904632568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaD80wiHxhby",
        "colab_type": "code",
        "colab": {},
        "outputId": "1c2e64b7-8ee9-4fcc-bb42-4f3e7b12cc0f"
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 32648), started 14:32:04 ago. (Use '!kill 32648' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-2a20f8d5e1cf1c79\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-2a20f8d5e1cf1c79\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 6006;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAsrbiUnxhb5",
        "colab_type": "text"
      },
      "source": [
        "# FAIL_03\n",
        "\n",
        "#### Observations\n",
        "* weights are initialised in range of -0.1 to 0.\n",
        "* we have used ReLu activation function, the hidden layer output becomes 0\n",
        "* no weight updation \n",
        "\n",
        "#### Possible_fixes\n",
        "* changing weights to range -0.1 to 0.1, ensures test acc of nearly 95%\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlswPaLYxhb6",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### task_04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teiIBQl5xhb7",
        "colab_type": "code",
        "colab": {},
        "outputId": "68e9d067-25eb-45ec-fbd3-c04da23352ca"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "\n",
        "\n",
        "# get the data\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256, seed=int(time()))\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the output layer\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    # I hear adding random noise to inputs helps with generalization!\n",
        "    img_batch = img_batch + tf.random.normal(tf.shape(img_batch), stddev=1)\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3025596141815186 Accuracy: 0.125\n",
            "Loss: 2.2998404502868652 Accuracy: 0.125\n",
            "Loss: 2.296457290649414 Accuracy: 0.12890625\n",
            "Starting new epoch...\n",
            "Loss: 2.299673318862915 Accuracy: 0.12109375\n",
            "Loss: 2.296933650970459 Accuracy: 0.125\n",
            "Starting new epoch...\n",
            "Loss: 2.3065595626831055 Accuracy: 0.07421875\n",
            "Loss: 2.2516214847564697 Accuracy: 0.203125\n",
            "Loss: 1.9676458835601807 Accuracy: 0.2734375\n",
            "Starting new epoch...\n",
            "Loss: 1.556753396987915 Accuracy: 0.46484375\n",
            "Loss: 1.1873114109039307 Accuracy: 0.56640625\n",
            "Starting new epoch...\n",
            "Loss: 1.2688671350479126 Accuracy: 0.51171875\n",
            "Loss: 1.0767525434494019 Accuracy: 0.609375\n",
            "Starting new epoch...\n",
            "Loss: 0.8752169609069824 Accuracy: 0.71875\n",
            "Loss: 0.9499244689941406 Accuracy: 0.65234375\n",
            "Loss: 0.8107883930206299 Accuracy: 0.75\n",
            "Starting new epoch...\n",
            "Loss: 0.8284597396850586 Accuracy: 0.6875\n",
            "Loss: 0.9085403084754944 Accuracy: 0.66796875\n",
            "Starting new epoch...\n",
            "Loss: 0.8686662912368774 Accuracy: 0.7109375\n",
            "Loss: 0.6725299954414368 Accuracy: 0.76171875\n",
            "Starting new epoch...\n",
            "Loss: 0.7745192050933838 Accuracy: 0.7421875\n",
            "Final test accuracy: 0.9140999913215637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KAruApdxhcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4Gv9JIPxhcR",
        "colab_type": "text"
      },
      "source": [
        "# FAIL_04\n",
        "\n",
        "#### Observations\n",
        "* during learning, due to the noise added images are little distorted\n",
        "\n",
        "#### Posssible_fixes\n",
        "* changing standard deviation(stddev) to 1 or 0.1 results in test acc of 91.4% and 93.4% each\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaD0Qg1VxhcS",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### task_05"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWas4ECMxhcU",
        "colab_type": "code",
        "colab": {},
        "outputId": "e665e5d9-5988-4ac4-db9e-ff75192e71d0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import datetime\n",
        "from datasets import MNISTDataset\n",
        "from time import time\n",
        "\n",
        "\n",
        "# get the data\n",
        "(train_imgs, train_lbls), (test_imgs, test_lbls) = tf.keras.datasets.mnist.load_data()\n",
        "mnist = MNISTDataset(train_imgs.reshape((-1, 784)), train_lbls,\n",
        "                     test_imgs.reshape((-1, 784)), test_lbls,\n",
        "                     batch_size=256, seed=int(time()))\n",
        "\n",
        "\n",
        "# define the model first, from input to output\n",
        "\n",
        "# let's use fewer layers...\n",
        "n_units = 100\n",
        "n_layers = 2\n",
        "\n",
        "# just set up a \"chain\" of hidden layers\n",
        "layers = []\n",
        "for layer in range(n_layers):\n",
        "    layers.append(tf.keras.layers.Dense(\n",
        "        n_units, activation=tf.nn.relu,\n",
        "        kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                         maxval=0.01),\n",
        "        bias_initializer=tf.initializers.constant(0.001)))\n",
        "\n",
        "# finally add the softmax output layer :))\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, \n",
        "    kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                     maxval=0.01)))\n",
        "\n",
        "lr = 0.1\n",
        "for step in range(2000):\n",
        "    img_batch, lbl_batch = mnist.next_batch()\n",
        "    with tf.GradientTape() as tape:\n",
        "        # here we just run all the layers in sequence via a for-loop\n",
        "        out = img_batch\n",
        "        for layer in layers:\n",
        "            out = layer(out)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=out, labels=lbl_batch))\n",
        "\n",
        "    weights = [var for l in layers for var in l.trainable_variables]\n",
        "    grads = tape.gradient(xent, weights)\n",
        "    for grad, var in zip(grads, weights):\n",
        "        var.assign_sub(lr*grad)\n",
        "\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch), tf.float32))\n",
        "        print(\"Loss: {} Accuracy: {}\".format(xent, acc))\n",
        "\n",
        "\n",
        "out = mnist.test_data\n",
        "for layer in layers:\n",
        "    out = layer(out)\n",
        "test_preds = tf.argmax(out, axis=1, output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, mnist.test_labels), tf.float32))\n",
        "print(\"Final test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.3026039600372314 Accuracy: 0.04296875\n",
            "Loss: 2.299373149871826 Accuracy: 0.125\n",
            "Loss: 2.299558162689209 Accuracy: 0.10546875\n",
            "Starting new epoch...\n",
            "Loss: 2.3000168800354004 Accuracy: 0.09765625\n",
            "Loss: 2.2941689491271973 Accuracy: 0.1015625\n",
            "Starting new epoch...\n",
            "Loss: 2.0937845706939697 Accuracy: 0.19140625\n",
            "Loss: 1.5501751899719238 Accuracy: 0.47265625\n",
            "Loss: 0.9017661809921265 Accuracy: 0.6640625\n",
            "Starting new epoch...\n",
            "Loss: 0.6427043080329895 Accuracy: 0.796875\n",
            "Loss: 0.6296447515487671 Accuracy: 0.796875\n",
            "Starting new epoch...\n",
            "Loss: 0.5869194865226746 Accuracy: 0.82421875\n",
            "Loss: 0.5490639209747314 Accuracy: 0.84765625\n",
            "Starting new epoch...\n",
            "Loss: 0.5242233276367188 Accuracy: 0.8515625\n",
            "Loss: 0.4126501679420471 Accuracy: 0.8671875\n",
            "Loss: 0.3858996033668518 Accuracy: 0.8984375\n",
            "Starting new epoch...\n",
            "Loss: 0.3020119071006775 Accuracy: 0.921875\n",
            "Loss: 0.2693336606025696 Accuracy: 0.93359375\n",
            "Starting new epoch...\n",
            "Loss: 0.33156073093414307 Accuracy: 0.890625\n",
            "Loss: 0.35210201144218445 Accuracy: 0.91015625\n",
            "Starting new epoch...\n",
            "Loss: 0.16621994972229004 Accuracy: 0.953125\n",
            "Final test accuracy: 0.929099977016449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Zt8ft4qxhca",
        "colab_type": "code",
        "colab": {},
        "outputId": "b0c00acf-c86a-43a1-f0ae-e199380800ff"
      },
      "source": [
        "# then load/run tensorboard\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 32648), started 14:50:00 ago. (Use '!kill 32648' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-16d3c1c1efee7f5d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-16d3c1c1efee7f5d\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          url.port = 6006;\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrFYbAjnxhch",
        "colab_type": "text"
      },
      "source": [
        "# FAIL_05\n",
        "\n",
        "\n",
        "#### Observations\n",
        "* The gradient becomes very less as the softmax activation is applied twice on the logits. This diminishes the benefit of -ve log likelihood loss and saturation occurs in the output.\n",
        "\n",
        "\n",
        "\n",
        "layers.append(tf.keras.layers.Dense(\n",
        "    10, activation=tf.nn.softmax,\n",
        "    kernel_initializer=tf.initializers.RandomUniform(minval=-0.01,\n",
        "                                                     maxval=0.01)))\n",
        "* due to this test acc is just above 11%\n",
        "\n",
        "#### Possible_changes\n",
        "* Remove the softmax activation at output layer, this willl lead to a test acc of 92.9%\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4w0S7oRxhci",
        "colab_type": "text"
      },
      "source": [
        "# An MLP training script using tf.data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nb1uMXuxhcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#from datasets import MNISTDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kURMrnExhcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDoR_hGtxhcw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# things we really gotta do:\n",
        "# - normalize the images to [0, 1] (first convert to float)\n",
        "# - reshape images from (28, 28) to (784,) (although we could do this later!)\n",
        "# - convert labels to int32 (otherwise tensorflow is gonna be sad :( )\n",
        "\n",
        "train_images = (train_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "test_images = (test_images.astype(np.float32) / 255.).reshape((-1, 784))\n",
        "\n",
        "train_labels = train_labels.astype(np.int32)\n",
        "test_labels = test_labels.astype(np.int32)\n",
        "\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "#train_data = train_data.batch(128)\n",
        "train_data = train_data.repeat(10)\n",
        "train_data = train_data.batch(128)\n",
        "\n",
        "train_data = train_data.shuffle(buffer_size=500)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55KGXTq5xhc5",
        "colab_type": "text"
      },
      "source": [
        "### tf.data API eliminates the use of training steps being explicitly called, by providing .repeat() function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZM--MCcxhc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.1\n",
        "W= tf.Variable(tf.random.uniform(shape = [784,50], minval=-0.1,maxval=0.1, dtype=tf.float32, seed=42))\n",
        "b = tf.Variable( tf.random.uniform(shape = [50], minval=-0.1,maxval=0.1, dtype=tf.float32, seed=42))\n",
        "\n",
        "W1= tf.Variable(tf.random.uniform(shape = [50,10], minval=-0.1,maxval=0.1, dtype=tf.float32, seed=42))\n",
        "b1 =tf.Variable( tf.random.uniform(shape = [10], minval=-0.1,maxval=0.1, dtype=tf.float32, seed=42))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNpMu0kixhc-",
        "colab_type": "text"
      },
      "source": [
        "#### **Dataset.shuffle** doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lK7B5nfxhc-",
        "colab_type": "code",
        "colab": {},
        "outputId": "aa7950b0-4366-4e89-c5f5-c3b0590e777f"
      },
      "source": [
        "#for step in range(2):\n",
        "i =0\n",
        "for img_batch, lbl_batch in train_data:\n",
        "    i=i+1\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = tf.matmul(tf.nn.relu(tf.matmul(img_batch, W) + b),W1) +b1 \n",
        "\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=lbl_batch))\n",
        "        \n",
        "        \n",
        "    dW, db, dW1, db1 = tape.gradient(xent, [W, b, W1, b1])    \n",
        "\n",
        "    W.assign_sub(learning_rate * dW)\n",
        "    b.assign_sub(learning_rate * db)\n",
        "    W1.assign_sub(learning_rate * dW1)\n",
        "    b1.assign_sub(learning_rate * db1)\n",
        "    \n",
        "        \n",
        "    if not i % 500:\n",
        "      preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "      acc = tf.reduce_mean(tf.cast(tf.equal(preds, lbl_batch),\n",
        "                             tf.float32))\n",
        "      print(\"Loss: {} Accuracy: {}\".format(xent, acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 0.26856154203414917 Accuracy: 0.9140625\n",
            "Loss: 0.1293889582157135 Accuracy: 0.9609375\n",
            "Loss: 0.35489124059677124 Accuracy: 0.9140625\n",
            "Loss: 0.14832685887813568 Accuracy: 0.9609375\n",
            "Loss: 0.20908045768737793 Accuracy: 0.9375\n",
            "Loss: 0.2550239562988281 Accuracy: 0.90625\n",
            "Loss: 0.1782987117767334 Accuracy: 0.921875\n",
            "Loss: 0.0496942438185215 Accuracy: 0.9765625\n",
            "Loss: 0.06334952265024185 Accuracy: 0.96875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHia8khyxhdD",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5fb671b-d7a2-4aa3-8f5e-ca0783eebddf"
      },
      "source": [
        "test_preds = tf.argmax( tf.matmul(tf.nn.relu(tf.matmul(test_images, W) + b),W1) +b1                \n",
        "                       , axis=1, output_type=tf.int32)\n",
        "acc1 = tf.reduce_mean(tf.cast(tf.equal(test_preds, test_labels),\n",
        "                             tf.float32))\n",
        "print(acc1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(0.9622, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rNeXkjjxhdG",
        "colab_type": "text"
      },
      "source": [
        "### Result of 6 type of Ordering\n",
        "| **Order**             |**Test Accuracy**|\n",
        "|-----------------------|---------------|\n",
        "| Shuffle,Batch,Repeat |       95.7%    |\n",
        "| Shuffle,Repeat,Batch |       96.1%   |\n",
        "| Batch,Repeat,Shuffle |       96.0%   |\n",
        "| Batch,Shuffle,Repeat |       95.8%   |\n",
        "| Repeat,Shuffle,Batch |       96.1%   |\n",
        "| Repeat,Batch,Shuffle |       96.2%   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdnTlSLUxhdH",
        "colab_type": "text"
      },
      "source": [
        "The optimum order of using these functions should consider these points:\n",
        "- Batch of data to be trained should be picked after shuffling to aviod bias issues\n",
        "- Repeat should be applied before shuffing because\n",
        " \n",
        " *.shuffle doesn't signal the end of an epoch until the shuffle buffer is empty. So a shuffle placed before a repeat will show every element of one epoch before moving to the next*\n",
        "\n",
        "---\n",
        "\n",
        "A more descriptive information is given here: [Repeat, Shuffle & Batch](https://stackoverflow.com/questions/53514495/what-does-batch-repeat-and-shuffle-do-with-tensorflow-dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s39T9YOxhdH",
        "colab_type": "text"
      },
      "source": [
        "#### The best order would be \n",
        "(1) Repeat \n",
        "(2) Shuffle\n",
        "(3) Batch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPL9vt_UxhdI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}